# Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning

ðŸ“Œ Paper: https://proceedings.mlr.press/v202/tang23a/tang23a.pdf

ðŸ“Œ Website: https://relational-autodiff.github.io/

## Directory Structure
- `Relational-Algebra-AD`
  - Auto-Differentiation library for relational algebra.
- `TestGCN`
  - A Graph Convolutional Networks (GCN) by relational algebra.

## News
- **[2023.12.08]** Our RA auto differentiation has been integrated with ![Amazon Redshift](https://aws.amazon.com/redshift/features/redshift-ml/)! ðŸ”¥ðŸ”¥ðŸ”¥

## Citation
Please consider citing the our paper if you find it helpful. Thank you!
```

@InProceedings{pmlr-v202-tang23a,
  title = 	 {Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning},
  author =       {Tang, Yuxin and Ding, Zhimin and Jankov, Dimitrije and Yuan, Binhang and Bourgeois, Daniel and Jermaine, Chris},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {33581--33598},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/tang23a/tang23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/tang23a.html},
  abstract = 	 {The relational data model was designed to facilitate large-scale data management and analytics. We consider the problem of how to differentiate computations expressed relationally. We show experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets, and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.}
}
```

## Acknowledgements
This repo is built upon the previous work [PlinyCompute](https://arxiv.org/abs/1711.05573) and [AutoDiff](https://github.com/autodiff/autodiff). Thanks for their wonderful works.
